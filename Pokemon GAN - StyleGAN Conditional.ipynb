{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0a3be7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# library to get dataloader\n",
    "from dataloaders import get_pkmn_dataloader\n",
    "\n",
    "# library to get loss functions\n",
    "from loss_functions import get_generator_loss_func,get_disc_loss_func,gradient_penalty,get_gradient\n",
    "\n",
    "# generators and discriminators\n",
    "from DCGeneratorCustom import DCGeneratorCustom\n",
    "from DCDiscriminatorCustom import DCDiscriminatorCustom\n",
    "from DCGeneratorStandard import DCGeneratorStandard\n",
    "from DCDiscriminatorStandard import DCDiscriminatorStandard\n",
    "from DCDiscriminatorStandardDropout import DCDiscriminatorStandardDropout\n",
    "from DiscriminatorPatchGAN import DiscriminatorPatchGAN,DiscriminatorPatchGANConditional\n",
    "from DCGeneratorConditionalLarge import DCGeneratorConditionalLarge\n",
    "\n",
    "from DCGeneratorConditional import DCGeneratorConditional\n",
    "from DCGeneratorConditionalLarge import DCGeneratorConditionalLarge\n",
    "from DCDiscriminatorConditional import DCDiscriminatorConditional\n",
    "from DCDiscriminatorConditionalLarge import DCDiscriminatorConditionalLarge\n",
    "from UNetArchitecture import UNet,UNetConditional,UNetConditionalImage\n",
    "from StyleGanGenerator import MicroStyleGANGeneratorConditional,MicroStyleGANGenerator\n",
    "\n",
    "# util methods\n",
    "from utils import get_noise\n",
    "\n",
    "# constants\n",
    "from pkmn_constants import PKMN_TYPES,CLASS_IDX_2_PKMN_TYPE,NUM_PKMN_TYPES,REDUCED_PKMN_TYPES,CLASS_IDX_2_PKMN_TYPE_REDUCED\n",
    "\n",
    "# whether to use CPU/GPU (pass this along everywhere)\n",
    "device_str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_str)\n",
    "\n",
    "print(\"Using device: {}\".format(device_str))\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a100e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "# if apply_denormalization is true, then we re-scale the images back \n",
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), denorm_transform = None, use_uniform_transform = False):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    \n",
    "    # We don't specifically need this since we are doing the denormalization ourself\n",
    "    \n",
    "    if denorm_transform is not None:\n",
    "      assert use_uniform_transform == False\n",
    "      image_tensor = denorm_transform(image_tensor)\n",
    "    if use_uniform_transform:\n",
    "      # cannot use both uniform and denorm transform together\n",
    "      assert denorm_transform == None\n",
    "      image_tensor = (image_tensor + 1) / 2 # scale from [-1, 1] to [0, 1] space\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "def save_model(model, output_filename):\n",
    "  torch.save(model.state_dict(), output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e75bbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moose_abdool/miniconda3/envs/cs236g_py37/lib/python3.7/site-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 64, 64])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                 [-1, 1024]          52,224\n",
      "              ReLU-2                 [-1, 1024]               0\n",
      "            Linear-3                 [-1, 1024]       1,049,600\n",
      "              ReLU-4                 [-1, 1024]               0\n",
      "            Linear-5                   [-1, 64]          65,600\n",
      "     MappingLayers-6                   [-1, 64]               0\n",
      "            Conv2d-7            [-1, 296, 4, 4]       1,364,264\n",
      "       InjectNoise-8            [-1, 296, 4, 4]             296\n",
      "         LeakyReLU-9            [-1, 296, 4, 4]               0\n",
      "   InstanceNorm2d-10            [-1, 296, 4, 4]               0\n",
      "           Linear-11                  [-1, 296]          19,240\n",
      "           Linear-12                  [-1, 296]          19,240\n",
      "            AdaIN-13            [-1, 296, 4, 4]               0\n",
      "MicroStyleGANGeneratorBlock-14            [-1, 296, 4, 4]               0\n",
      "         Upsample-15            [-1, 296, 8, 8]               0\n",
      "           Conv2d-16            [-1, 296, 8, 8]         788,840\n",
      "      InjectNoise-17            [-1, 296, 8, 8]             296\n",
      "        LeakyReLU-18            [-1, 296, 8, 8]               0\n",
      "   InstanceNorm2d-19            [-1, 296, 8, 8]               0\n",
      "           Linear-20                  [-1, 296]          19,240\n",
      "           Linear-21                  [-1, 296]          19,240\n",
      "            AdaIN-22            [-1, 296, 8, 8]               0\n",
      "MicroStyleGANGeneratorBlock-23            [-1, 296, 8, 8]               0\n",
      "         Upsample-24          [-1, 296, 16, 16]               0\n",
      "           Conv2d-25          [-1, 296, 16, 16]         788,840\n",
      "      InjectNoise-26          [-1, 296, 16, 16]             296\n",
      "        LeakyReLU-27          [-1, 296, 16, 16]               0\n",
      "   InstanceNorm2d-28          [-1, 296, 16, 16]               0\n",
      "           Linear-29                  [-1, 296]          19,240\n",
      "           Linear-30                  [-1, 296]          19,240\n",
      "            AdaIN-31          [-1, 296, 16, 16]               0\n",
      "MicroStyleGANGeneratorBlock-32          [-1, 296, 16, 16]               0\n",
      "         Upsample-33          [-1, 296, 32, 32]               0\n",
      "           Conv2d-34          [-1, 296, 32, 32]         788,840\n",
      "      InjectNoise-35          [-1, 296, 32, 32]             296\n",
      "        LeakyReLU-36          [-1, 296, 32, 32]               0\n",
      "   InstanceNorm2d-37          [-1, 296, 32, 32]               0\n",
      "           Linear-38                  [-1, 296]          19,240\n",
      "           Linear-39                  [-1, 296]          19,240\n",
      "            AdaIN-40          [-1, 296, 32, 32]               0\n",
      "MicroStyleGANGeneratorBlock-41          [-1, 296, 32, 32]               0\n",
      "         Upsample-42          [-1, 296, 64, 64]               0\n",
      "           Conv2d-43          [-1, 296, 64, 64]         788,840\n",
      "      InjectNoise-44          [-1, 296, 64, 64]             296\n",
      "        LeakyReLU-45          [-1, 296, 64, 64]               0\n",
      "   InstanceNorm2d-46          [-1, 296, 64, 64]               0\n",
      "           Linear-47                  [-1, 296]          19,240\n",
      "           Linear-48                  [-1, 296]          19,240\n",
      "            AdaIN-49          [-1, 296, 64, 64]               0\n",
      "MicroStyleGANGeneratorBlock-50          [-1, 296, 64, 64]               0\n",
      "           Conv2d-51            [-1, 3, 64, 64]             891\n",
      "================================================================\n",
      "Total params: 5,881,819\n",
      "Trainable params: 5,881,819\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 86.36\n",
      "Params size (MB): 22.44\n",
      "Estimated Total Size (MB): 108.80\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "z_dim = 32\n",
    "map_hidden_dim = 1024\n",
    "w_dim = 64\n",
    "in_chan = 512\n",
    "out_chan = 3\n",
    "gen_kernel_size = 3\n",
    "hidden_chan =  296\n",
    "use_class_embed_gen = False\n",
    "class_embed_gen_size = 32\n",
    "use_class_style = False\n",
    "class_style_embed_size = 4\n",
    "\n",
    "# 512 hidden channel si about 16m parameters\n",
    "mu_stylegan = MicroStyleGANGeneratorConditional(\n",
    "    z_dim=z_dim, \n",
    "    map_hidden_dim=map_hidden_dim,\n",
    "    w_dim=w_dim,\n",
    "    in_chan=in_chan,\n",
    "    out_chan=out_chan, \n",
    "    kernel_size=gen_kernel_size, \n",
    "    hidden_chan=hidden_chan,\n",
    "    use_class_embed = use_class_embed_gen,\n",
    "    class_embed_size = class_embed_gen_size,\n",
    "    use_class_style = use_class_style,\n",
    "    class_style_embed_size = class_style_embed_size,\n",
    "    output_dim = 64\n",
    ")\n",
    "\n",
    "test_samples = 10\n",
    "test_result = mu_stylegan(get_noise(test_samples, z_dim), torch.ones(test_samples, 1))\n",
    "\n",
    "print(test_result.shape)\n",
    "\n",
    "summary(mu_stylegan, [(1,32), (1,1)], device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c60af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of patchgan output is: torch.Size([7, 1, 6, 6])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1                   [-1, 16]             288\n",
      "            Conv2d-2           [-1, 32, 96, 96]             160\n",
      "   FeatureMapBlock-3           [-1, 32, 96, 96]               0\n",
      "            Conv2d-4           [-1, 64, 96, 96]          18,496\n",
      "         LeakyReLU-5           [-1, 64, 96, 96]               0\n",
      "            Conv2d-6           [-1, 64, 96, 96]          36,928\n",
      "         LeakyReLU-7           [-1, 64, 96, 96]               0\n",
      "         MaxPool2d-8           [-1, 64, 48, 48]               0\n",
      "  ContractingBlock-9           [-1, 64, 48, 48]               0\n",
      "           Conv2d-10          [-1, 128, 48, 48]          73,856\n",
      "      BatchNorm2d-11          [-1, 128, 48, 48]             256\n",
      "        LeakyReLU-12          [-1, 128, 48, 48]               0\n",
      "           Conv2d-13          [-1, 128, 48, 48]         147,584\n",
      "      BatchNorm2d-14          [-1, 128, 48, 48]             256\n",
      "        LeakyReLU-15          [-1, 128, 48, 48]               0\n",
      "        MaxPool2d-16          [-1, 128, 24, 24]               0\n",
      " ContractingBlock-17          [-1, 128, 24, 24]               0\n",
      "           Conv2d-18          [-1, 256, 24, 24]         295,168\n",
      "      BatchNorm2d-19          [-1, 256, 24, 24]             512\n",
      "        LeakyReLU-20          [-1, 256, 24, 24]               0\n",
      "           Conv2d-21          [-1, 256, 24, 24]         590,080\n",
      "      BatchNorm2d-22          [-1, 256, 24, 24]             512\n",
      "        LeakyReLU-23          [-1, 256, 24, 24]               0\n",
      "        MaxPool2d-24          [-1, 256, 12, 12]               0\n",
      " ContractingBlock-25          [-1, 256, 12, 12]               0\n",
      "           Conv2d-26          [-1, 512, 12, 12]       1,180,160\n",
      "      BatchNorm2d-27          [-1, 512, 12, 12]           1,024\n",
      "        LeakyReLU-28          [-1, 512, 12, 12]               0\n",
      "           Conv2d-29          [-1, 512, 12, 12]       2,359,808\n",
      "      BatchNorm2d-30          [-1, 512, 12, 12]           1,024\n",
      "        LeakyReLU-31          [-1, 512, 12, 12]               0\n",
      "        MaxPool2d-32            [-1, 512, 6, 6]               0\n",
      " ContractingBlock-33            [-1, 512, 6, 6]               0\n",
      "           Conv2d-34              [-1, 1, 6, 6]             513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moose_abdool/miniconda3/envs/cs236g_py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ph/g131tglj70v_hdzxdjcz34p00000gn/T/ipykernel_15198/3840465005.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of patchgan output is: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatchgan_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_patchgan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m96\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/cs236g_py37/lib/python3.7/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# assume 4 bytes/number (float on cuda).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mtotal_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mtotal_output_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# x2 for gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mtotal_params_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236g_py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0;32m-> 3052\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   3053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236g_py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'tuple'"
     ]
    }
   ],
   "source": [
    "# What are the differences here?\n",
    "# PatchGan input is (3x96x96)\n",
    "# output is 6x6 patches if input dim is 96\n",
    "# output is 8x8 patches if input dim is 128\n",
    "# Basically, it's easy to change the shape of the input dimension image, we just create more patches\n",
    "\n",
    "test_patchgan = DiscriminatorPatchGANConditional(input_channels = 3, hidden_channels = 32)\n",
    "\n",
    "patchgan_out = test_patchgan(torch.ones(7, 3, 96, 96), torch.ones(7, 1))\n",
    "\n",
    "print(\"Shape of patchgan output is: {}\".format(patchgan_out.shape))\n",
    "\n",
    "summary(test_patchgan, [(3, 96, 96), (1,1)], device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e784feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Define the basic hyper-parameters =====\n",
    "\n",
    "\n",
    "# training epochs\n",
    "n_epochs = 250\n",
    "\n",
    "# # generator settings\n",
    "z_dim = 16\n",
    "map_hidden_dim = 1024\n",
    "w_dim = 128\n",
    "in_chan = 512\n",
    "out_chan = 3\n",
    "gen_kernel_size = 3\n",
    "hidden_chan =  200\n",
    "use_class_embed_gen = False\n",
    "class_embed_gen_size = 16\n",
    "use_class_style = True # check if this was preventing convergence ?\n",
    "class_style_embed_size = 4\n",
    "\n",
    "# discriminator settings\n",
    "use_patchgan_disc=True\n",
    "hidden_dim_disc = 25\n",
    "use_class_embed_disc = True\n",
    "disc_class_embed_size = 16\n",
    "use_dropout_disc = False\n",
    "disc_dropout_prob = 0.1\n",
    "use_gaussian_noise = True\n",
    "gaussian_noise_std = 0.01 # using 0.1 seems too high here for patchgan\n",
    "patchgan_dropout = 0.25\n",
    "use_class_proj = False\n",
    "\n",
    "# general parameters - batch size and dataloader\n",
    "batch_size = 32 #cublas allocation error is related to batch size being too large ?\n",
    "dataloader_name = \"conditional_64_no_shiny_mainclass_flip_rotate_standard_norm\"\n",
    "# \"conditional_64_dim_mainclass_with_shiny_and_back_flip_rotate_custom_norm\"\n",
    "num_workers = 1\n",
    "\n",
    "#\"conditional_64_no_shiny_mainclass_flip_rotate_standard_norm\"\n",
    "\n",
    "\n",
    "\n",
    "# how often to display images and debug info. Basically, the numerator is how many images you want to\n",
    "# process before showing some debug info\n",
    "display_step = int((4000 / batch_size))\n",
    "periodic_saving = False\n",
    "# after how many epochs do you save the model?\n",
    "epoch_save_step = 20\n",
    "save_prefix = \"experiment_15/test_stylegan_conditional_with_class_adapative_and_dcdisc\"\n",
    "imgs_to_display = 10\n",
    "\n",
    "print(\"Planning to display images every {} steps\".format(display_step))\n",
    "\n",
    "# other sources say 0.00275 works better...but the DCGAn paper used 0.0002 (ie. about 10-4 instead of 10-3)\n",
    "gen_lr = 0.0002 # 0.0002 works for both, but takes at least 10-15 epochs before anything interesting happens?\n",
    "disc_lr = 0.0002\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "disc_repeats = 1\n",
    "gen_repeats = 1\n",
    "\n",
    "# loss functions\n",
    "use_wgan_loss = False\n",
    "if use_wgan_loss:\n",
    "  gen_loss_func = get_generator_loss_func(\"wgan_gen_loss\")\n",
    "  disc_loss_func = get_disc_loss_func(\"wgan_disc_loss\")\n",
    "  c_lambda = 10\n",
    "else:\n",
    "  if use_patchgan_disc:\n",
    "    gen_loss_func = get_generator_loss_func(\"basic_gen_loss_with_logits\") # basic_gen_loss_with_logits\n",
    "    disc_loss_func = get_disc_loss_func(\"noisy_patchgan_disc_loss\") # noisy_patchgan_disc_loss\n",
    "  else:\n",
    "    gen_loss_func = get_generator_loss_func(\"basic_gen_loss\")\n",
    "    disc_loss_func = get_disc_loss_func(\"noisy_disc_loss\")    \n",
    "\n",
    "assert imgs_to_display <= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader, based on the appropriate batch size. \n",
    "\n",
    "use_reduced_types = True\n",
    "show_preview = True\n",
    "\n",
    "pkmn_dataloader, denorm_transform = get_pkmn_dataloader(dataloader_name, batch_size,num_workers=num_workers)\n",
    "test_size = 10\n",
    "\n",
    "# show a batch before and after denorm\n",
    "test_data_iter = iter(pkmn_dataloader)\n",
    "test_images, test_labels = next(test_data_iter)\n",
    "\n",
    "print(\"length of dataset (number of steps) is: {}, total size is: {}\".format(len(pkmn_dataloader), len(pkmn_dataloader)*batch_size))\n",
    "\n",
    "if show_preview:\n",
    "  show_tensor_images(test_images[0:test_size], num_images = test_size, size = (3,64,64), denorm_transform = None)\n",
    "  show_tensor_images(test_images[0:test_size], num_images = test_size, size = (3,64,64), denorm_transform = denorm_transform)\n",
    "\n",
    "if use_reduced_types:\n",
    "  valid_types = REDUCED_PKMN_TYPES\n",
    "  class_idx_to_type = CLASS_IDX_2_PKMN_TYPE_REDUCED\n",
    "else:\n",
    "  valid_types = PKMN_TYPES\n",
    "  class_idx_to_type = CLASS_IDX_2_PKMN_TYPE\n",
    "  \n",
    "num_pkmn_types = len(valid_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator/discriminator and use them\n",
    "# layer initialization for Generator and Discriminator (for Conv2d and ConvTranpose2d)\n",
    "\n",
    "gen = MicroStyleGANGeneratorConditional(\n",
    "    z_dim=z_dim, \n",
    "    map_hidden_dim=map_hidden_dim,\n",
    "    w_dim=w_dim,\n",
    "    in_chan=in_chan,\n",
    "    out_chan=out_chan, \n",
    "    kernel_size=gen_kernel_size, \n",
    "    hidden_chan=hidden_chan,\n",
    "    use_class_embed = use_class_embed_gen,\n",
    "    class_embed_size = class_embed_gen_size,\n",
    "    use_class_style = use_class_style,\n",
    "    class_style_embed_size = class_style_embed_size,\n",
    "    output_dim = 64\n",
    ").to(device)\n",
    "\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=gen_lr, betas=(beta_1, beta_2))\n",
    "\n",
    "if use_patchgan_disc:\n",
    "    print(\"Using patchgan discriminator!\")\n",
    "    disc = DiscriminatorPatchGANConditional(hidden_channels = hidden_dim_disc,              \n",
    "                                            use_dropout = use_dropout_disc, \n",
    "                                            input_image_dim = 64,\n",
    "                                            dropout_prob = patchgan_dropout,\n",
    "                                            class_embed_size = disc_class_embed_size,\n",
    "                                            use_gaussian_noise = use_gaussian_noise,\n",
    "                                            gaussian_noise_std = gaussian_noise_std,\n",
    "                                            use_class_proj = use_class_proj).to(device)\n",
    "\n",
    "else:\n",
    "    print(\"Using dc discriminator!\")\n",
    "    disc = DCDiscriminatorConditional(hidden_dim = hidden_dim_disc, class_embed_size=disc_class_embed_size,\n",
    "                                    use_dropout = use_dropout_disc, early_dropout=early_dropout, mid_dropout = mid_dropout,\n",
    "                                    late_dropout = late_dropout, use_gaussian_noise = use_gaussian_noise,\n",
    "                                    gaussian_noise_std = gaussian_noise_std,\n",
    "                                    use_class_proj = use_class_proj).to(device)\n",
    "\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=disc_lr, betas=(beta_1, beta_2))\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)\n",
    "\n",
    "#summary(gen, [(7, z_dim), (1,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59280606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "discriminator_fake_preds = [] # median value of P(real) the discriminator predicts on the fake images (per batch)\n",
    "discriminator_real_preds = [] # median value of P(real) the discriminator predicts on the real images (per batch)\n",
    "\n",
    "if use_patchgan_disc:\n",
    "    agg_func = nn.Sigmoid() # use this if you have logit outputs\n",
    "else:\n",
    "    agg_func = nn.Identity()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the (real_images, labels)\n",
    "    for real, true_labels in tqdm(pkmn_dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        real = real.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "        \n",
    "        mean_iteration_discriminator_loss = 0\n",
    "        for _ in range(disc_repeats):\n",
    "            ### Update discriminator ###\n",
    "            disc_opt.zero_grad()\n",
    "            fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "            fake = gen(fake_noise, true_labels)\n",
    "            \n",
    "            disc_fake_pred = disc(fake.detach(), true_labels)\n",
    "            disc_real_pred = disc(real, true_labels)\n",
    "\n",
    "            \n",
    "            if use_wgan_loss:\n",
    "              epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n",
    "              gradient = get_gradient(disc, real, true_labels, fake.detach(), epsilon)\n",
    "              gp = gradient_penalty(gradient)\n",
    "              # how should we track these two relative losses ?\n",
    "              disc_loss = disc_loss_func(disc_fake_pred, disc_real_pred, gp, c_lambda)              \n",
    "\n",
    "            else:\n",
    "              # compute discriminator loss normally\n",
    "              disc_loss = disc_loss_func(disc_fake_pred, disc_real_pred, device)\n",
    "            \n",
    "            # Keep track of the average discriminator loss in this batch\n",
    "            mean_iteration_discriminator_loss += disc_loss.item() / disc_repeats\n",
    "            \n",
    "            # Update gradients\n",
    "            # when using WGAN, we have retain_graph = True, but it probably takes longer\n",
    "            disc_loss.backward(retain_graph=use_wgan_loss)\n",
    "            \n",
    "            # Update optimizer\n",
    "            disc_opt.step()\n",
    "            \n",
    "        discriminator_losses += [mean_iteration_discriminator_loss]\n",
    "        # notice this only takes the last one from the iteration (if you run the discriminator multiple times)\n",
    "        # use a Sigmoid here to go from logits back into the P(real) space\n",
    "        discriminator_fake_preds += [agg_func(torch.mean(disc_fake_pred).detach())]\n",
    "        discriminator_real_preds += [agg_func(torch.mean(disc_real_pred).detach())]\n",
    "\n",
    "        ### Update generator ###\n",
    "        mean_iteration_gen_loss = 0\n",
    "        for _ in range(gen_repeats):\n",
    "          gen_opt.zero_grad()\n",
    "          fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)\n",
    "          fake_2 = gen(fake_noise_2, true_labels)\n",
    "          disc_fake_pred = disc(fake_2, true_labels)\n",
    "\n",
    "          # compute gen loss\n",
    "          gen_loss = gen_loss_func(disc_fake_pred, device)\n",
    "          \n",
    "          mean_iteration_gen_loss += gen_loss.item() / gen_repeats\n",
    "          \n",
    "          gen_loss.backward()\n",
    "\n",
    "          # Update the weights\n",
    "          gen_opt.step()\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        generator_losses += [mean_iteration_gen_loss]\n",
    "        \n",
    "        ### Visualization code ###\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
    "            # maybe print out the last values here ? To see how stable it is overtime ? These all seem to be very close to 0.5 / 1\n",
    "            disc_prediction_real = sum(discriminator_real_preds[-display_step:]) / display_step\n",
    "            disc_prediction_fake = sum(discriminator_fake_preds[-display_step:]) / display_step\n",
    "            print(f\"Step {cur_step}: Epoch: {epoch}: Generator loss: {gen_mean}, discriminator loss: {disc_mean} mean disc pred on real images: {disc_prediction_real}, fake images: {disc_prediction_fake}\")\n",
    "            show_tensor_images(fake[0:imgs_to_display], imgs_to_display, size = (3,64,64), denorm_transform = denorm_transform)\n",
    "            show_tensor_images(real[0:imgs_to_display], imgs_to_display, size = (3,64,64), denorm_transform = denorm_transform)\n",
    "            \n",
    "            ground_truth_types = [class_idx_to_type[class_idx] for class_idx in true_labels[0:imgs_to_display].cpu().numpy()]\n",
    "            print(\"Pokemon types we are trying to generate are: {} \\n    \\t \\t \\t \\t \\t {}\".format(ground_truth_types[0:5], ground_truth_types[5:]))\n",
    "            \n",
    "            step_bins = 20\n",
    "            \n",
    "            # todo: add proper labels to this plot\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Generator Loss\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"discriminator Loss\"\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        # increase the current step (ie. one batch)\n",
    "        cur_step += 1\n",
    "        \n",
    "    if periodic_saving and epoch % epoch_save_step == 0 and epoch_save_step > 0:\n",
    "      outfile_name = \"{}_{}.pt\".format(save_prefix, cur_step)\n",
    "      print(\"===== Saving intermediate model with name {} ! ====\".format(outfile_name))\n",
    "      save_model(gen, outfile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f627c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "\n",
    "out_file = \"second_gan_with_standard_dcgan_arch_after_149epochs.pt\"\n",
    "  \n",
    "save_model(gen, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da493fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for inference and generate some random classes\n",
    "\n",
    "gen.load_state_dict(torch.load(\"{}\".format(out_file)))\n",
    "\n",
    "# Performing inference - actually, maybe I ran some other cell after this so it's not really well defined\n",
    "num_samples = 10\n",
    "\n",
    "sample_vector = get_noise(num_samples, z_dim, device)\n",
    "random_classes = torch.randint(low = 0, high = NUM_PKMN_TYPES, size = (num_samples,)).to(device)\n",
    "\n",
    "fake_images = gen(sample_vector, random_classes)\n",
    "\n",
    "output_size = gen.output_dim\n",
    "# + 0.9\n",
    "show_tensor_images(fake_images, num_images=num_samples, size=(1,256, 256), use_uniform_transform = True, denorm_transform = None)\n",
    "\n",
    "\n",
    "pkmn_classes = [CLASS_IDX_2_PKMN_TYPE[class_idx] for class_idx in random_classes.cpu().numpy()]\n",
    "print(\"Pokemon types we are trying to generate are: {} \\n    \\t \\t \\t \\t \\t {}\".format(pkmn_classes[0:5], pkmn_classes[5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples of a specific type from the model\n",
    "target_type = \"Fire\"\n",
    "class_idx = PKMN_TYPES.index(target_type)\n",
    "\n",
    "class_tiled = torch.from_numpy(np.array([class_idx] * num_samples))\n",
    "\n",
    "sample_vector = get_noise(num_samples, z_dim, device)\n",
    "classes = class_tiled.to(device)\n",
    "\n",
    "fake_images = gen(sample_vector, classes)\n",
    "\n",
    "output_size = gen.output_dim\n",
    "# + 0.9\n",
    "show_tensor_images(fake_images, num_images=num_samples, size=(1,256, 256), use_uniform_transform = True, denorm_transform = None)\n",
    "\n",
    "\n",
    "pkmn_classes = [CLASS_IDX_2_PKMN_TYPE[class_idx] for class_idx in classes.cpu().numpy()]\n",
    "print(\"Pokemon types we are trying to generate are: {} \\n    \\t \\t \\t \\t \\t {}\".format(pkmn_classes[0:5], pkmn_classes[5:]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
