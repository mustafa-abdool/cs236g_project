{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a3be7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# library to get dataloader\n",
    "from dataloaders import get_pkmn_dataloader\n",
    "\n",
    "# library to get loss functions\n",
    "from loss_functions import get_generator_loss_func,get_disc_loss_func,gradient_penalty,get_gradient\n",
    "\n",
    "# generators and discriminators\n",
    "from DCGeneratorCustom import DCGeneratorCustom\n",
    "from DCDiscriminatorCustom import DCDiscriminatorCustom\n",
    "from DCGeneratorStandard import DCGeneratorStandard\n",
    "from DCDiscriminatorStandard import DCDiscriminatorStandard\n",
    "from DCDiscriminatorStandardDropout import DCDiscriminatorStandardDropout\n",
    "from DiscriminatorPatchGAN import DiscriminatorPatchGAN,DiscriminatorPatchGANConditional\n",
    "from DCGeneratorConditionalLarge import DCGeneratorConditionalLarge\n",
    "\n",
    "from DCGeneratorConditional import DCGeneratorConditional\n",
    "from DCGeneratorConditionalLarge import DCGeneratorConditionalLarge\n",
    "from DCDiscriminatorConditional import DCDiscriminatorConditional\n",
    "from DCDiscriminatorConditionalLarge import DCDiscriminatorConditionalLarge\n",
    "from UNetArchitecture import UNet,UNetConditional,UNetConditionalImage\n",
    "from StyleGanGenerator import MicroStyleGANGeneratorConditional,MicroStyleGANGenerator\n",
    "\n",
    "# util methods\n",
    "from utils import get_noise\n",
    "\n",
    "# constants\n",
    "from pkmn_constants import PKMN_TYPES,CLASS_IDX_2_PKMN_TYPE,NUM_PKMN_TYPES,REDUCED_PKMN_TYPES,CLASS_IDX_2_PKMN_TYPE_REDUCED\n",
    "\n",
    "# whether to use CPU/GPU (pass this along everywhere)\n",
    "device_str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_str)\n",
    "\n",
    "print(\"Using device: {}\".format(device_str))\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a100e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "# if apply_denormalization is true, then we re-scale the images back \n",
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), denorm_transform = None, use_uniform_transform = False):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    \n",
    "    # We don't specifically need this since we are doing the denormalization ourself\n",
    "    \n",
    "    if denorm_transform is not None:\n",
    "      assert use_uniform_transform == False\n",
    "      image_tensor = denorm_transform(image_tensor)\n",
    "    if use_uniform_transform:\n",
    "      # cannot use both uniform and denorm transform together\n",
    "      assert denorm_transform == None\n",
    "      image_tensor = (image_tensor + 1) / 2 # scale from [-1, 1] to [0, 1] space\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "def save_model(model, output_filename):\n",
    "  torch.save(model.state_dict(), output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139bf81c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape of unet is: torch.Size([7, 3, 64, 64])\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1             [-1, 1, 1, 16]             288\n",
      "            Conv2d-2           [-1, 12, 64, 64]              36\n",
      "   FeatureMapBlock-3           [-1, 12, 64, 64]               0\n",
      "            Conv2d-4           [-1, 24, 64, 64]           2,616\n",
      "       BatchNorm2d-5           [-1, 24, 64, 64]              48\n",
      "           Dropout-6           [-1, 24, 64, 64]               0\n",
      "         LeakyReLU-7           [-1, 24, 64, 64]               0\n",
      "            Conv2d-8           [-1, 24, 64, 64]           5,208\n",
      "       BatchNorm2d-9           [-1, 24, 64, 64]              48\n",
      "          Dropout-10           [-1, 24, 64, 64]               0\n",
      "        LeakyReLU-11           [-1, 24, 64, 64]               0\n",
      "        MaxPool2d-12           [-1, 24, 32, 32]               0\n",
      " ContractingBlock-13           [-1, 24, 32, 32]               0\n",
      "           Conv2d-14           [-1, 48, 32, 32]          10,416\n",
      "      BatchNorm2d-15           [-1, 48, 32, 32]              96\n",
      "          Dropout-16           [-1, 48, 32, 32]               0\n",
      "        LeakyReLU-17           [-1, 48, 32, 32]               0\n",
      "           Conv2d-18           [-1, 48, 32, 32]          20,784\n",
      "      BatchNorm2d-19           [-1, 48, 32, 32]              96\n",
      "          Dropout-20           [-1, 48, 32, 32]               0\n",
      "        LeakyReLU-21           [-1, 48, 32, 32]               0\n",
      "        MaxPool2d-22           [-1, 48, 16, 16]               0\n",
      " ContractingBlock-23           [-1, 48, 16, 16]               0\n",
      "           Conv2d-24           [-1, 96, 16, 16]          41,568\n",
      "      BatchNorm2d-25           [-1, 96, 16, 16]             192\n",
      "          Dropout-26           [-1, 96, 16, 16]               0\n",
      "        LeakyReLU-27           [-1, 96, 16, 16]               0\n",
      "           Conv2d-28           [-1, 96, 16, 16]          83,040\n",
      "      BatchNorm2d-29           [-1, 96, 16, 16]             192\n",
      "          Dropout-30           [-1, 96, 16, 16]               0\n",
      "        LeakyReLU-31           [-1, 96, 16, 16]               0\n",
      "        MaxPool2d-32             [-1, 96, 8, 8]               0\n",
      " ContractingBlock-33             [-1, 96, 8, 8]               0\n",
      "           Conv2d-34            [-1, 192, 8, 8]         166,080\n",
      "      BatchNorm2d-35            [-1, 192, 8, 8]             384\n",
      "        LeakyReLU-36            [-1, 192, 8, 8]               0\n",
      "           Conv2d-37            [-1, 192, 8, 8]         331,968\n",
      "      BatchNorm2d-38            [-1, 192, 8, 8]             384\n",
      "        LeakyReLU-39            [-1, 192, 8, 8]               0\n",
      "        MaxPool2d-40            [-1, 192, 4, 4]               0\n",
      " ContractingBlock-41            [-1, 192, 4, 4]               0\n",
      "           Conv2d-42            [-1, 384, 4, 4]         663,936\n",
      "      BatchNorm2d-43            [-1, 384, 4, 4]             768\n",
      "        LeakyReLU-44            [-1, 384, 4, 4]               0\n",
      "           Conv2d-45            [-1, 384, 4, 4]       1,327,488\n",
      "      BatchNorm2d-46            [-1, 384, 4, 4]             768\n",
      "        LeakyReLU-47            [-1, 384, 4, 4]               0\n",
      "        MaxPool2d-48            [-1, 384, 2, 2]               0\n",
      " ContractingBlock-49            [-1, 384, 2, 2]               0\n",
      "           Conv2d-50            [-1, 768, 2, 2]       2,654,976\n",
      "      BatchNorm2d-51            [-1, 768, 2, 2]           1,536\n",
      "        LeakyReLU-52            [-1, 768, 2, 2]               0\n",
      "           Conv2d-53            [-1, 768, 2, 2]       5,309,184\n",
      "      BatchNorm2d-54            [-1, 768, 2, 2]           1,536\n",
      "        LeakyReLU-55            [-1, 768, 2, 2]               0\n",
      "        MaxPool2d-56            [-1, 768, 1, 1]               0\n",
      " ContractingBlock-57            [-1, 768, 1, 1]               0\n",
      "         Upsample-58            [-1, 768, 2, 2]               0\n",
      "           Conv2d-59            [-1, 384, 1, 1]       1,180,032\n",
      "           Conv2d-60            [-1, 384, 1, 1]       2,654,592\n",
      "      BatchNorm2d-61            [-1, 384, 1, 1]             768\n",
      "             ReLU-62            [-1, 384, 1, 1]               0\n",
      "           Conv2d-63            [-1, 384, 2, 2]         590,208\n",
      "      BatchNorm2d-64            [-1, 384, 2, 2]             768\n",
      "             ReLU-65            [-1, 384, 2, 2]               0\n",
      "   ExpandingBlock-66            [-1, 384, 2, 2]               0\n",
      "         Upsample-67            [-1, 384, 4, 4]               0\n",
      "           Conv2d-68            [-1, 192, 3, 3]         295,104\n",
      "           Conv2d-69            [-1, 192, 3, 3]         663,744\n",
      "      BatchNorm2d-70            [-1, 192, 3, 3]             384\n",
      "             ReLU-71            [-1, 192, 3, 3]               0\n",
      "           Conv2d-72            [-1, 192, 4, 4]         147,648\n",
      "      BatchNorm2d-73            [-1, 192, 4, 4]             384\n",
      "             ReLU-74            [-1, 192, 4, 4]               0\n",
      "   ExpandingBlock-75            [-1, 192, 4, 4]               0\n",
      "         Upsample-76            [-1, 192, 8, 8]               0\n",
      "           Conv2d-77             [-1, 96, 7, 7]          73,824\n",
      "           Conv2d-78             [-1, 96, 7, 7]         165,984\n",
      "      BatchNorm2d-79             [-1, 96, 7, 7]             192\n",
      "             ReLU-80             [-1, 96, 7, 7]               0\n",
      "           Conv2d-81             [-1, 96, 8, 8]          36,960\n",
      "      BatchNorm2d-82             [-1, 96, 8, 8]             192\n",
      "             ReLU-83             [-1, 96, 8, 8]               0\n",
      "   ExpandingBlock-84             [-1, 96, 8, 8]               0\n",
      "         Upsample-85           [-1, 96, 16, 16]               0\n",
      "           Conv2d-86           [-1, 48, 15, 15]          18,480\n",
      "           Conv2d-87           [-1, 48, 15, 15]          41,520\n",
      "      BatchNorm2d-88           [-1, 48, 15, 15]              96\n",
      "             ReLU-89           [-1, 48, 15, 15]               0\n",
      "           Conv2d-90           [-1, 48, 16, 16]           9,264\n",
      "      BatchNorm2d-91           [-1, 48, 16, 16]              96\n",
      "             ReLU-92           [-1, 48, 16, 16]               0\n",
      "   ExpandingBlock-93           [-1, 48, 16, 16]               0\n",
      "         Upsample-94           [-1, 48, 32, 32]               0\n",
      "           Conv2d-95           [-1, 24, 31, 31]           4,632\n",
      "           Conv2d-96           [-1, 24, 31, 31]          10,392\n",
      "      BatchNorm2d-97           [-1, 24, 31, 31]              48\n",
      "             ReLU-98           [-1, 24, 31, 31]               0\n",
      "           Conv2d-99           [-1, 24, 32, 32]           2,328\n",
      "     BatchNorm2d-100           [-1, 24, 32, 32]              48\n",
      "            ReLU-101           [-1, 24, 32, 32]               0\n",
      "  ExpandingBlock-102           [-1, 24, 32, 32]               0\n",
      "        Upsample-103           [-1, 24, 64, 64]               0\n",
      "          Conv2d-104           [-1, 12, 63, 63]           1,164\n",
      "          Conv2d-105           [-1, 12, 63, 63]           2,604\n",
      "     BatchNorm2d-106           [-1, 12, 63, 63]              24\n",
      "            ReLU-107           [-1, 12, 63, 63]               0\n",
      "          Conv2d-108           [-1, 12, 64, 64]             588\n",
      "     BatchNorm2d-109           [-1, 12, 64, 64]              24\n",
      "            ReLU-110           [-1, 12, 64, 64]               0\n",
      "  ExpandingBlock-111           [-1, 12, 64, 64]               0\n",
      "          Conv2d-112            [-1, 3, 64, 64]              39\n",
      " FeatureMapBlock-113            [-1, 3, 64, 64]               0\n",
      "            Tanh-114            [-1, 3, 64, 64]               0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moose_abdool/miniconda3/envs/cs236g_py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:86: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ph/g131tglj70v_hdzxdjcz34p00000gn/T/ipykernel_66449/2696286363.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Output shape of unet is: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_unet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/cs236g_py37/lib/python3.7/site-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# assume 4 bytes/number (float on cuda).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     \u001b[0mtotal_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m     \u001b[0mtotal_output_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtotal_output\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# x2 for gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mtotal_params_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mprod\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236g_py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mprod\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   3050\u001b[0m     \"\"\"\n\u001b[1;32m   3051\u001b[0m     return _wrapreduction(a, np.multiply, 'prod', axis, dtype, out,\n\u001b[0;32m-> 3052\u001b[0;31m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[0m\u001b[1;32m   3053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/cs236g_py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'tuple'"
     ]
    }
   ],
   "source": [
    "# Test out the conditional Unet\n",
    "test_unet = UNetConditionalImage(input_channels = 2, output_channels = 3, \n",
    "                            hidden_channels = 12, input_dim=64, \n",
    "                            use_class_embed=True, \n",
    "                            class_embed_size=16, use_conditional_layer_arch=False)\n",
    "\n",
    "out = test_unet(torch.ones(7, 1, 64, 64), torch.ones(7, 1))\n",
    "\n",
    "print(\"Output shape of unet is: {}\".format(out.shape))\n",
    "\n",
    "summary(test_unet, [(1, 64, 64), (1, 1)], device = 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the differences here?\n",
    "# PatchGan input is (3x96x96)\n",
    "# output is 6x6 patches if input dim is 96\n",
    "# output is 8x8 patches if input dim is 128\n",
    "# Basically, it's easy to change the shape of the input dimension image, we just create more patches\n",
    "\n",
    "test_patchgan = DiscriminatorPatchGANConditional(input_channels = 3, hidden_channels = 2)\n",
    "\n",
    "patchgan_out = test_patchgan(torch.ones(7, 3, 96, 96), torch.ones(7, 1))\n",
    "\n",
    "print(\"Shape of patchgan output is: {}\".format(patchgan_out.shape))\n",
    "\n",
    "summary(test_patchgan, [(3, 96, 96), (1,1)], device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e784feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Planning to display images every 100 steps\n"
     ]
    }
   ],
   "source": [
    "# ======= Define the basic hyper-parameters =====\n",
    "\n",
    "\n",
    "# training epochs\n",
    "n_epochs = 100\n",
    "\n",
    "# dimension of noise vector\n",
    "z_dim = 64 - 18\n",
    "# dimension of input image size\n",
    "gen_input_image_size = 64\n",
    "# hidden dimensions\n",
    "hidden_dim_gen = 12 # was 10\n",
    "hidden_dim_disc = 32 # was 64\n",
    "disc_class_embed_size = 2\n",
    "batch_size = 40 # 64 works\n",
    "# how often to display images and debug info. Basically, the numerator is how many images you want to\n",
    "# process before showing some debug info\n",
    "display_step = int((4000 / batch_size))\n",
    "periodic_saving = True\n",
    "# after how many epochs do you save the model?\n",
    "epoch_save_step = 20\n",
    "save_prefix = \"experiment_10/test_unet_and_dcdiscriminator\"\n",
    "imgs_to_display = 10\n",
    "\n",
    "print(\"Planning to display images every {} steps\".format(display_step))\n",
    "\n",
    "# other sources say 0.00275 works better...but the DCGAn paper used 0.0002 (ie. about 10-4 instead of 10-3)\n",
    "gen_lr = 0.0002 # 0.0002 works for both, but takes at least 10-15 epochs before anything interesting happens?\n",
    "disc_lr = 0.0002\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "disc_repeats = 1\n",
    "gen_repeats = 1\n",
    "\n",
    "# loss functions\n",
    "use_wgan_loss = False\n",
    "if use_wgan_loss:\n",
    "    # not performannt enough\n",
    "  gen_loss_func = get_generator_loss_func(\"wgan_gen_loss\")\n",
    "  disc_loss_func = get_disc_loss_func(\"wgan_disc_loss\")\n",
    "  c_lambda = 10\n",
    "else:\n",
    "  gen_loss_func = get_generator_loss_func(\"basic_gen_loss\") #get_generator_loss_func(\"basic_gen_loss_with_logits\")\n",
    "  disc_loss_func = get_disc_loss_func(\"noisy_disc_loss\") #get_disc_loss_func(\"noisy_patchgan_disc_loss\") #can also do noisy here\n",
    "\n",
    "assert imgs_to_display <= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader, based on the appropriate batch size. \n",
    "\n",
    "use_reduced_types = True\n",
    "show_preview = True\n",
    "dataloader_name = \"conditional_64_no_shiny_mainclass_flip_rotate_standard_norm\"\n",
    "\n",
    "pkmn_dataloader, denorm_transform = get_pkmn_dataloader(dataloader_name, batch_size,num_workers=0)\n",
    "test_size = 10\n",
    "\n",
    "# show a batch before and after denorm\n",
    "test_data_iter = iter(pkmn_dataloader)\n",
    "test_images, test_labels = next(test_data_iter)\n",
    "\n",
    "print(\"length of dataset (number of steps) is: {}, total size is: {}\".format(len(pkmn_dataloader), len(pkmn_dataloader)*batch_size))\n",
    "\n",
    "if show_preview:\n",
    "  show_tensor_images(test_images[0:test_size], num_images = test_size, size = (3,64,64), denorm_transform = None)\n",
    "  show_tensor_images(test_images[0:test_size], num_images = test_size, size = (3,64,64), denorm_transform = denorm_transform)\n",
    "\n",
    "if use_reduced_types:\n",
    "  valid_types = REDUCED_PKMN_TYPES\n",
    "  class_idx_to_type = CLASS_IDX_2_PKMN_TYPE_REDUCED\n",
    "else:\n",
    "  valid_types = PKMN_TYPES\n",
    "  class_idx_to_type = CLASS_IDX_2_PKMN_TYPE\n",
    "  \n",
    "num_pkmn_types = len(valid_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a1243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator/discriminator and use them\n",
    "# layer initialization for Generator and Discriminator (for Conv2d and ConvTranpose2d)\n",
    "\n",
    "#gen = DCGeneratorConditionalLarge(z_dim = z_dim, hidden_dim = hidden_dim_gen, output_dim = 96, use_class_embed = False, class_embed_size = 16).to(device)\n",
    "#UNetConditional(z_dim = z_dim, hidden_channels= hidden_dim_gen, input_dim = 64, use_dropout = False, dropout_prob = 0.1).to(device)\n",
    "\n",
    "gen = UNetConditionalImage(input_channels = 2, output_channels = 3, \n",
    "                            hidden_channels = gen_input_image_size, input_dim=gen_input_image_size, \n",
    "                            z_dim = z_dim, use_class_embed=True, \n",
    "                            class_embed_size=16, use_conditional_layer_arch=False).to(device)\n",
    "\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=gen_lr, betas=(beta_1, beta_2))\n",
    "disc = DCDiscriminatorConditional(hidden_dim = hidden_dim_disc, class_embed_size=disc_class_embed_size,\n",
    "                                early_dropout=0.5, mid_dropout = 0.5, late_dropout = 0.5).to(device)\n",
    "# DiscriminatorPatchGANConditional(hidden_channels = hidden_dim_disc, use_dropout = True, dropout_prob = 0.5).to(device) \n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=disc_lr, betas=(beta_1, beta_2))\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59280606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "discriminator_fake_preds = [] # median value of P(real) the discriminator predicts on the fake images (per batch)\n",
    "discriminator_real_preds = [] # median value of P(real) the discriminator predicts on the real images (per batch)\n",
    "sigmoid_function = nn.Sigmoid() # use this if you have logit outputs\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the (real_images, labels)\n",
    "    for real, true_labels in tqdm(pkmn_dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        real = real.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "        \n",
    "        mean_iteration_discriminator_loss = 0\n",
    "        for _ in range(disc_repeats):\n",
    "            ### Update discriminator ###\n",
    "            disc_opt.zero_grad()\n",
    "            fake_noise = get_image_noise(cur_batch_size, gen_input_image_size, device) #get_noise(cur_batch_size, z_dim, device=device)\n",
    "            fake = gen(fake_noise, true_labels)\n",
    "            \n",
    "            disc_fake_pred = disc(fake.detach(), true_labels)\n",
    "            disc_real_pred = disc(real, true_labels)\n",
    "\n",
    "            \n",
    "            if use_wgan_loss:\n",
    "              epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n",
    "              gradient = get_gradient(disc, real, true_labels, fake.detach(), epsilon)\n",
    "              gp = gradient_penalty(gradient)\n",
    "              # how should we track these two relative losses ?\n",
    "              disc_loss = disc_loss_func(disc_fake_pred, disc_real_pred, gp, c_lambda)              \n",
    "\n",
    "            else:\n",
    "              # compute discriminator loss normally\n",
    "              disc_loss = disc_loss_func(disc_fake_pred, disc_real_pred, device)\n",
    "            \n",
    "            # Keep track of the average discriminator loss in this batch\n",
    "            mean_iteration_discriminator_loss += disc_loss.item() / disc_repeats\n",
    "            \n",
    "            # Update gradients\n",
    "            # when using WGAN, we have retain_graph = True, but it probably takes longer\n",
    "            disc_loss.backward(retain_graph=use_wgan_loss)\n",
    "            \n",
    "            # Update optimizer\n",
    "            disc_opt.step()\n",
    "            \n",
    "        discriminator_losses += [mean_iteration_discriminator_loss]\n",
    "        # notice this only takes the last one from the iteration (if you run the discriminator multiple times)\n",
    "        # use a Sigmoid here to go from logits back into the P(real) space\n",
    "        discriminator_fake_preds += [torch.mean(disc_fake_pred).detach()]\n",
    "        discriminator_real_preds += [torch.mean(disc_real_pred).detach()]\n",
    "\n",
    "        ### Update generator ###\n",
    "        mean_iteration_gen_loss = 0\n",
    "        for _ in range(gen_repeats):\n",
    "          gen_opt.zero_grad()\n",
    "          fake_noise_2 = get_image_noise(cur_batch_size, gen_input_image_size, device) #get_noise(cur_batch_size, z_dim, device=device)\n",
    "          fake_2 = gen(fake_noise_2, true_labels)\n",
    "          disc_fake_pred = disc(fake_2, true_labels)\n",
    "\n",
    "          # compute gen loss\n",
    "          gen_loss = gen_loss_func(disc_fake_pred, device)\n",
    "          \n",
    "          mean_iteration_gen_loss += gen_loss.item() / gen_repeats\n",
    "          \n",
    "          gen_loss.backward()\n",
    "\n",
    "          # Update the weights\n",
    "          gen_opt.step()\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        generator_losses += [mean_iteration_gen_loss]\n",
    "        \n",
    "        ### Visualization code ###\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
    "            # maybe print out the last values here ? To see how stable it is overtime ? These all seem to be very close to 0.5 / 1\n",
    "            disc_prediction_real = sum(discriminator_real_preds[-display_step:]) / display_step\n",
    "            disc_prediction_fake = sum(discriminator_fake_preds[-display_step:]) / display_step\n",
    "            print(f\"Step {cur_step}: Epoch: {epoch}: Generator loss: {gen_mean}, discriminator loss: {disc_mean} mean disc pred on real images: {disc_prediction_real}, fake images: {disc_prediction_fake}\")\n",
    "            show_tensor_images(fake[0:imgs_to_display], imgs_to_display, size = (3,64,64), denorm_transform = denorm_transform)\n",
    "            show_tensor_images(real[0:imgs_to_display], imgs_to_display, size = (3,64,64), denorm_transform = denorm_transform)\n",
    "            \n",
    "            ground_truth_types = [class_idx_to_type[class_idx] for class_idx in true_labels[0:imgs_to_display].cpu().numpy()]\n",
    "            print(\"Pokemon types we are trying to generate are: {} \\n    \\t \\t \\t \\t \\t {}\".format(ground_truth_types[0:5], ground_truth_types[5:]))\n",
    "            \n",
    "            step_bins = 20\n",
    "            \n",
    "            # todo: add proper labels to this plot\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Generator Loss\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"discriminator Loss\"\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        # increase the current step (ie. one batch)\n",
    "        cur_step += 1\n",
    "        \n",
    "    if periodic_saving and epoch % epoch_save_step == 0 and epoch_save_step > 0:\n",
    "      outfile_name = \"{}_{}.pt\".format(save_prefix, cur_step)\n",
    "      print(\"===== Saving intermediate model with name {} ! ====\".format(outfile_name))\n",
    "      save_model(gen, outfile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f627c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "\n",
    "out_file = \"second_gan_with_standard_dcgan_arch_after_149epochs.pt\"\n",
    "  \n",
    "save_model(gen, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da493fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for inference and generate some random classes\n",
    "\n",
    "gen.load_state_dict(torch.load(\"{}\".format(out_file)))\n",
    "\n",
    "# Performing inference - actually, maybe I ran some other cell after this so it's not really well defined\n",
    "num_samples = 10\n",
    "\n",
    "sample_vector = get_noise(num_samples, z_dim, device)\n",
    "random_classes = torch.randint(low = 0, high = num_pkmn_types, size = (num_samples,)).to(device)\n",
    "\n",
    "fake_images = gen(sample_vector, random_classes)\n",
    "\n",
    "output_size = gen.output_dim\n",
    "# + 0.9\n",
    "show_tensor_images(fake_images, num_images=num_samples, size=(1,256, 256), use_uniform_transform = True, denorm_transform = None)\n",
    "\n",
    "\n",
    "pkmn_classes = [class_idx_to_type[class_idx] for class_idx in random_classes.cpu().numpy()]\n",
    "print(\"Pokemon types we are trying to generate are: {} \\n    \\t \\t \\t \\t \\t {}\".format(pkmn_classes[0:5], pkmn_classes[5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples of a specific type from the model\n",
    "target_type = \"Fire\"\n",
    "class_idx = valid_types.index(target_type)\n",
    "\n",
    "class_tiled = torch.from_numpy(np.array([class_idx] * num_samples))\n",
    "\n",
    "sample_vector = get_noise(num_samples, z_dim, device)\n",
    "classes = class_tiled.to(device)\n",
    "\n",
    "fake_images = gen(sample_vector, classes)\n",
    "\n",
    "output_size = gen.output_dim\n",
    "# + 0.9\n",
    "show_tensor_images(fake_images, num_images=num_samples, size=(1,256, 256), use_uniform_transform = True, denorm_transform = None)\n",
    "\n",
    "\n",
    "pkmn_classes = [class_idx_to_type[class_idx] for class_idx in classes.cpu().numpy()]\n",
    "print(\"Pokemon types we are trying to generate are: {} \\n    \\t \\t \\t \\t \\t {}\".format(pkmn_classes[0:5], pkmn_classes[5:]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
