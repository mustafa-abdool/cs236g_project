{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3be7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0) # Set for testing purposes, please do not change!\n",
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "from GaussianNoise import decay_gauss_std,set_gauss_std\n",
    "\n",
    "\n",
    "# library to get dataloader\n",
    "from dataloaders import get_pkmn_dataloader\n",
    "\n",
    "# library to get loss functions\n",
    "from loss_functions import get_generator_loss_func,get_disc_loss_func,gradient_penalty,get_gradient\n",
    "\n",
    "# generators and discriminators\n",
    "from DCGeneratorCustom import DCGeneratorCustom\n",
    "from DCDiscriminatorCustom import DCDiscriminatorCustom\n",
    "from DCGeneratorStandard import DCGeneratorStandard\n",
    "from DCDiscriminatorStandard import DCDiscriminatorStandard\n",
    "from DCDiscriminatorStandardDropout import DCDiscriminatorStandardDropout\n",
    "from DiscriminatorPatchGAN import DiscriminatorPatchGAN,DiscriminatorPatchGANConditional\n",
    "from DCGeneratorConditionalLarge import DCGeneratorConditionalLarge\n",
    "\n",
    "from DCGeneratorConditional import DCGeneratorConditional\n",
    "from DCGeneratorConditionalLarge import DCGeneratorConditionalLarge\n",
    "from DCDiscriminatorConditional import DCDiscriminatorConditional\n",
    "from DCDiscriminatorConditionalLarge import DCDiscriminatorConditionalLarge\n",
    "from UNetArchitecture import UNet,UNetConditional,UNetConditionalImage\n",
    "\n",
    "# util methods\n",
    "from utils import get_noise,get_image_noise\n",
    "\n",
    "# constants\n",
    "from pkmn_constants import PKMN_TYPES,CLASS_IDX_2_PKMN_TYPE,NUM_PKMN_TYPES,REDUCED_PKMN_TYPES,CLASS_IDX_2_PKMN_TYPE_REDUCED\n",
    "\n",
    "# whether to use CPU/GPU (pass this along everywhere)\n",
    "device_str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device_str)\n",
    "\n",
    "print(\"Using device: {}\".format(device_str))\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a100e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "# if apply_denormalization is true, then we re-scale the images back \n",
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), denorm_transform = None, use_uniform_transform = False):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    \n",
    "    # We don't specifically need this since we are doing the denormalization ourself\n",
    "    \n",
    "    if denorm_transform is not None:\n",
    "      assert use_uniform_transform == False\n",
    "      image_tensor = denorm_transform(image_tensor)\n",
    "    if use_uniform_transform:\n",
    "      # cannot use both uniform and denorm transform together\n",
    "      assert denorm_transform == None\n",
    "      image_tensor = (image_tensor + 1) / 2 # scale from [-1, 1] to [0, 1] space\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "\n",
    "def save_model(model, output_filename):\n",
    "  torch.save(model.state_dict(), output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139bf81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the conditional Unet with image input (to get # of parameters)\n",
    "test_unet =UNetConditionalImage(input_channels = 2, output_channels = 3, \n",
    "                            hidden_channels = 12, input_dim=64, \n",
    "                            use_class_embed=False, \n",
    "                            class_embed_size=16, use_conditional_layer_arch=False, vocab_size=16,\n",
    "                            inject_noise = True, use_class_adapt_layer = True)\n",
    "\n",
    "out = test_unet(torch.ones(7, 1, 64, 64), torch.ones(7, 1))\n",
    "\n",
    "print(\"Output shape of unet is: {}\".format(out.shape))\n",
    "\n",
    "summary(test_unet, [(1, 64, 64), (1, 1)], device = 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28197ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the conditional Unet with vector + tiling input(to get # of parameters)\n",
    "test_unet = UNetConditional(input_channels = 1, output_channels = 3, \n",
    "                            hidden_channels = 8, input_dim=64, \n",
    "                            use_class_embed=True, z_dim = 16,\n",
    "                            class_embed_size=16, use_conditional_layer_arch=False, vocab_size=16,\n",
    "                            use_mapping_network = True, map_network_hidden_size = 1024\n",
    "                            )\n",
    "\n",
    "out = test_unet(torch.ones(7, 16), torch.ones(7, 1))\n",
    "\n",
    "print(\"Output shape of unet is: {}\".format(out.shape))\n",
    "\n",
    "summary(test_unet, [(1, 16), (1, 1)], device = 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0a821f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the differences here?\n",
    "# PatchGan input is (3x96x96)\n",
    "# output is 6x6 patches if input dim is 96\n",
    "# output is 8x8 patches if input dim is 128\n",
    "# Basically, it's easy to change the shape of the input dimension image, we just create more patches\n",
    "\n",
    "test_patchgan = DiscriminatorPatchGANConditional(input_channels = 3, hidden_channels = 2, input_image_dim=64,\n",
    "                                                use_class_proj = True)\n",
    "\n",
    "patchgan_out = test_patchgan(torch.ones(7, 3, 64, 64), torch.ones(7, 1))\n",
    "\n",
    "print(\"Shape of patchgan output is: {}\".format(patchgan_out.shape))\n",
    "\n",
    "summary(test_patchgan, [(3, 64, 64), (1,1)], device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e784feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======= Define the basic hyper-parameters =====\n",
    "\n",
    "\n",
    "# training epochs\n",
    "n_epochs = 250\n",
    "\n",
    "# Generator parameters\n",
    "use_unet_image = False\n",
    "use_unet_conditional = True\n",
    "\n",
    "assert use_unet_image != use_unet_conditional\n",
    "\n",
    "\n",
    "hidden_dim_gen = 12\n",
    "z_dim = 20 # dimension of noise vector (if using UNetConditional)\n",
    "gen_input_image_size = 64 # dimension of input image size (if using UNetConditionalImage)\n",
    "use_noise_upsampling = False\n",
    "original_noise_dim = 32\n",
    "use_gen_class_embed = True\n",
    "gen_class_embed_size = 12\n",
    "use_mapping_network = False\n",
    "map_network_hidden_size = 1024 # only used in UnetConditional\n",
    "use_conditional_layer_arch = True\n",
    "use_gen_dropout = False\n",
    "dropout_gen_prob = 0.1\n",
    "vocab_size = 13\n",
    "\n",
    "# noise and adapative class layer\n",
    "use_gen_noise = False # whether or not to inject noise to gen layers using the class\n",
    "use_class_ada_layer = True\n",
    "class_adapt_layer_embed_size = 32\n",
    "use_middle_noise = True\n",
    "middle_noise_std = 0.02\n",
    "\n",
    "\n",
    "# Discriminator Parameters\n",
    "use_patchgan_disc=False\n",
    "use_dropout_disc = True\n",
    "patchgan_dropout = 0.1\n",
    "hidden_dim_disc = 55 # was 64\n",
    "disc_class_embed_size = 4\n",
    "use_class_embed_disc = True\n",
    "early_dropout = 0.4\n",
    "mid_dropout = 0.2\n",
    "late_dropout = 0.1\n",
    "use_gaussian_noise = True\n",
    "gaussian_noise_std = 0.1\n",
    "use_class_proj = True\n",
    "\n",
    "\n",
    "# General Parameters - batch size and dataset\n",
    "batch_size = 64 # 64 works\n",
    "#\n",
    "dataloader_name = \"conditional_64_no_shiny_mainclass_flip_rotate_standard_norm\"\n",
    "#\"conditional_64_dim_no_shiny_with_flip_and_rotate_and_standard_norm\"\n",
    "#\"conditional_64_dim_mainclass_with_shiny_flip_rotate_custom_norm\"\n",
    "\n",
    "# how often to display images and debug info. Basically, the numerator is how many images you want to\n",
    "# process before showing some debug info\n",
    "display_step = int((4000 / batch_size))\n",
    "periodic_saving = True\n",
    "# after how many epochs do you save the model?\n",
    "epoch_save_step = 20\n",
    "save_prefix = \"experiment_16/test_unet_and_dcdiscriminator_class_proj_conditional_layer\"\n",
    "imgs_to_display = 10\n",
    "\n",
    "print(\"Planning to display images every {} steps\".format(display_step))\n",
    "\n",
    "# other sources say 0.00275 works better...but the DCGAn paper used 0.0002 (ie. about 10-4 instead of 10-3)\n",
    "gen_lr = 0.0002 # 0.0002 works for both, but takes at least 10-15 epochs before anything interesting happens?\n",
    "disc_lr = 0.0002\n",
    "beta_1 = 0.5\n",
    "beta_2 = 0.999\n",
    "disc_repeats = 1\n",
    "gen_repeats = 1\n",
    "\n",
    "# loss functions used\n",
    "use_wgan_loss = False\n",
    "if use_wgan_loss:\n",
    "    # not performannt enough\n",
    "  gen_loss_func = get_generator_loss_func(\"wgan_gen_loss\")\n",
    "  disc_loss_func = get_disc_loss_func(\"wgan_disc_loss\")\n",
    "  c_lambda = 10\n",
    "else:\n",
    "  if use_patchgan_disc:\n",
    "    gen_loss_func = get_generator_loss_func(\"basic_gen_loss_with_logits\") # basic_gen_loss_with_logits\n",
    "    disc_loss_func = get_disc_loss_func(\"noisy_patchgan_disc_loss\") # noisy_patchgan_disc_loss\n",
    "  else:\n",
    "    gen_loss_func = get_generator_loss_func(\"basic_gen_loss\")\n",
    "    disc_loss_func = get_disc_loss_func(\"noisy_disc_loss\")    \n",
    "\n",
    "assert imgs_to_display <= batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataloader, based on the appropriate batch size. \n",
    "\n",
    "use_reduced_types = True\n",
    "show_preview = True\n",
    "\n",
    "pkmn_dataloader, denorm_transform = get_pkmn_dataloader(dataloader_name, batch_size,num_workers=0)\n",
    "test_size = 10\n",
    "\n",
    "# show a batch before and after denorm\n",
    "test_data_iter = iter(pkmn_dataloader)\n",
    "test_images, test_labels = next(test_data_iter)\n",
    "\n",
    "print(\"length of dataset (number of steps) is: {}, total size is: {}\".format(len(pkmn_dataloader), len(pkmn_dataloader)*batch_size))\n",
    "\n",
    "if show_preview:\n",
    "  show_tensor_images(test_images[0:test_size], num_images = test_size, size = (3,64,64), denorm_transform = None)\n",
    "  show_tensor_images(test_images[0:test_size], num_images = test_size, size = (3,64,64), denorm_transform = denorm_transform)\n",
    "\n",
    "if use_reduced_types:\n",
    "  valid_types = REDUCED_PKMN_TYPES\n",
    "  class_idx_to_type = CLASS_IDX_2_PKMN_TYPE_REDUCED\n",
    "else:\n",
    "  valid_types = PKMN_TYPES\n",
    "  class_idx_to_type = CLASS_IDX_2_PKMN_TYPE\n",
    "  \n",
    "num_pkmn_types = len(valid_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a1243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator/discriminator and use them\n",
    "# layer initialization for Generator and Discriminator (for Conv2d and ConvTranpose2d)\n",
    "#gen = DCGeneratorConditionalLarge(z_dim = z_dim, hidden_dim = hidden_dim_gen, output_dim = 96, use_class_embed = False, class_embed_size = 16).to(device)\n",
    "\n",
    "\n",
    "if use_unet_image:\n",
    "  print(\"Using image unet\")\n",
    "  gen = UNetConditionalImage(input_channels = 2, output_channels = 3, \n",
    "                             hidden_channels = hidden_dim_gen, input_dim=gen_input_image_size, \n",
    "                             use_class_embed=use_gen_class_embed, class_embed_size=gen_class_embed_size,\n",
    "                             vocab_size=num_pkmn_types,\n",
    "                             use_conditional_layer_arch=use_conditional_layer_arch,# not sure if this makes sense here ?\n",
    "                             use_mapping_network = use_mapping_network, map_network_hidden_size = map_network_hidden_size,\n",
    "                             inject_noise = use_gen_noise, use_class_adapt_layer = use_class_ada_layer,\n",
    "                             use_middle_noise = use_middle_noise, middle_noise_std = middle_noise_std).to(device)\n",
    "\n",
    "else:\n",
    "  print(\"Using basic conditional Unet!\")\n",
    "  gen = UNetConditional(z_dim = z_dim, \n",
    "                        hidden_channels= hidden_dim_gen,\n",
    "                        input_dim = 64, \n",
    "                        use_dropout = use_gen_dropout, dropout_prob = dropout_gen_prob,\n",
    "                        vocab_size = num_pkmn_types,\n",
    "                        use_class_embed=use_gen_class_embed, class_embed_size=gen_class_embed_size,\n",
    "                        use_mapping_network = use_mapping_network, map_network_hidden_size = map_network_hidden_size,\n",
    "                        inject_noise = use_gen_noise, use_class_adapt_layer = use_class_ada_layer,\n",
    "                        class_adapt_layer_embed_size = class_adapt_layer_embed_size\n",
    "                        ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if use_patchgan_disc:\n",
    "    print(\"Using patchgan discriminator!\")\n",
    "    disc = DiscriminatorPatchGANConditional(hidden_channels = hidden_dim_disc,              \n",
    "                                            use_dropout = use_dropout_disc, \n",
    "                                            input_image_dim = 64,\n",
    "                                            dropout_prob = patchgan_dropout,\n",
    "                                            class_embed_size = disc_class_embed_size,\n",
    "                                            use_gaussian_noise = use_gaussian_noise,\n",
    "                                            gaussian_noise_std = gaussian_noise_std,\n",
    "                                            use_class_proj = use_class_proj).to(device)\n",
    "\n",
    "else:\n",
    "    print(\"Using dc discriminator!\")\n",
    "    disc = DCDiscriminatorConditional(hidden_dim = hidden_dim_disc, class_embed_size=disc_class_embed_size,\n",
    "                                      use_dropout = use_dropout_disc, early_dropout=early_dropout, \n",
    "                                      mid_dropout = mid_dropout,\n",
    "                                      late_dropout = late_dropout, use_gaussian_noise = use_gaussian_noise,\n",
    "                                      gaussian_noise_std = gaussian_noise_std,\n",
    "                                      use_class_proj = use_class_proj).to(device)  \n",
    "  \n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=gen_lr, betas=(beta_1, beta_2))\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=disc_lr, betas=(beta_1, beta_2))\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)\n",
    "\n",
    "\n",
    "#summary(disc, [(3, 64, 64), (1,1)], device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59280606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "discriminator_fake_preds = [] # median value of P(real) the discriminator predicts on the fake images (per batch)\n",
    "discriminator_real_preds = [] # median value of P(real) the discriminator predicts on the real images (per batch)\n",
    "if use_patchgan_disc:\n",
    "    agg_func = nn.Sigmoid() # use this if you have logit outputs\n",
    "else:\n",
    "    agg_func = nn.Identity()\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the (real_images, labels)\n",
    "    for real, true_labels in tqdm(pkmn_dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        real = real.to(device)\n",
    "        true_labels = true_labels.to(device)\n",
    "        \n",
    "        mean_iteration_discriminator_loss = 0\n",
    "        for _ in range(disc_repeats):\n",
    "            ### Update discriminator ###\n",
    "            disc_opt.zero_grad()\n",
    "            if use_unet_image:\n",
    "              fake_noise = get_image_noise(cur_batch_size, gen_input_image_size, device, upsampling = use_noise_upsampling,noise_dim = original_noise_dim) #get_noise(cur_batch_size, z_dim, device=device)\n",
    "            else:\n",
    "              fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "\n",
    "            fake = gen(fake_noise, true_labels)\n",
    "            \n",
    "            disc_fake_pred = disc(fake.detach(), true_labels)\n",
    "            disc_real_pred = disc(real, true_labels)\n",
    "\n",
    "            \n",
    "            if use_wgan_loss:\n",
    "              epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n",
    "              gradient = get_gradient(disc, real, true_labels, fake.detach(), epsilon)\n",
    "              gp = gradient_penalty(gradient)\n",
    "              # how should we track these two relative losses ?\n",
    "              disc_loss = disc_loss_func(disc_fake_pred, disc_real_pred, gp, c_lambda)              \n",
    "\n",
    "            else:\n",
    "              # compute discriminator loss normally\n",
    "              disc_loss = disc_loss_func(disc_fake_pred, disc_real_pred, device)\n",
    "            \n",
    "            # Keep track of the average discriminator loss in this batch\n",
    "            mean_iteration_discriminator_loss += disc_loss.item() / disc_repeats\n",
    "            \n",
    "            # Update gradients\n",
    "            # when using WGAN, we have retain_graph = True, but it probably takes longer\n",
    "            disc_loss.backward(retain_graph=use_wgan_loss)\n",
    "            \n",
    "            # Update optimizer\n",
    "            disc_opt.step()\n",
    "            \n",
    "        discriminator_losses += [mean_iteration_discriminator_loss]\n",
    "        # notice this only takes the last one from the iteration (if you run the discriminator multiple times)\n",
    "        # use a Sigmoid here to go from logits back into the P(real) space\n",
    "        discriminator_fake_preds += [agg_func(torch.mean(disc_fake_pred).detach())]\n",
    "        discriminator_real_preds += [agg_func(torch.mean(disc_real_pred).detach())]\n",
    "\n",
    "        ### Update generator ###\n",
    "        mean_iteration_gen_loss = 0\n",
    "        for _ in range(gen_repeats):\n",
    "          gen_opt.zero_grad()\n",
    "          if use_unet_image:\n",
    "            fake_noise_2 = get_image_noise(cur_batch_size, gen_input_image_size, device, upsampling = use_noise_upsampling,noise_dim = original_noise_dim) #get_noise(cur_batch_size, z_dim, device=device)\n",
    "          else:\n",
    "            fake_noise_2 = get_noise(cur_batch_size, z_dim, device=device)          \n",
    "          fake_2 = gen(fake_noise_2, true_labels)\n",
    "          disc_fake_pred = disc(fake_2, true_labels)\n",
    "\n",
    "          # compute gen loss\n",
    "          gen_loss = gen_loss_func(disc_fake_pred, device)\n",
    "          \n",
    "          mean_iteration_gen_loss += gen_loss.item() / gen_repeats\n",
    "          \n",
    "          gen_loss.backward()\n",
    "\n",
    "          # Update the weights\n",
    "          gen_opt.step()\n",
    "\n",
    "        # Keep track of the average generator loss\n",
    "        generator_losses += [mean_iteration_gen_loss]\n",
    "        \n",
    "        ### Visualization code ###\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
    "            # maybe print out the last values here ? To see how stable it is overtime ? These all seem to be very close to 0.5 / 1\n",
    "            disc_prediction_real = sum(discriminator_real_preds[-display_step:]) / display_step\n",
    "            disc_prediction_fake = sum(discriminator_fake_preds[-display_step:]) / display_step\n",
    "            print(f\"Step {cur_step}: Epoch: {epoch}: Generator loss: {gen_mean}, discriminator loss: {disc_mean} mean disc pred on real images: {disc_prediction_real}, fake images: {disc_prediction_fake}\")\n",
    "            show_tensor_images(fake[0:imgs_to_display], imgs_to_display, size = (3,64,64), denorm_transform = denorm_transform)\n",
    "            show_tensor_images(real[0:imgs_to_display], imgs_to_display, size = (3,64,64), denorm_transform = denorm_transform)\n",
    "            \n",
    "            ground_truth_types = [class_idx_to_type[class_idx] for class_idx in true_labels[0:imgs_to_display].cpu().numpy()]\n",
    "            print(\"Pokemon types we are trying to generate are: {} \\n    \\t \\t \\t \\t \\t {}\".format(ground_truth_types[0:5], ground_truth_types[5:]))\n",
    "            \n",
    "            step_bins = 20\n",
    "            \n",
    "            # todo: add proper labels to this plot\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Generator Loss\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"discriminator Loss\"\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        # increase the current step (ie. one batch)\n",
    "        cur_step += 1\n",
    "        \n",
    "    if periodic_saving and epoch % epoch_save_step == 0 and epoch_save_step > 0:\n",
    "      outfile_name = \"{}_{}.pt\".format(save_prefix, cur_step)\n",
    "      print(\"===== Saving intermediate model with name {} ! ====\".format(outfile_name))\n",
    "      save_model(gen, outfile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f627c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "\n",
    "out_file = \"experiment_11/unet_and_dcgan_exp_11_fulltrain.pt\"\n",
    "  \n",
    "save_model(gen, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b3224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model for inference and to generate samples (note: GANs use train mode still)\n",
    "\n",
    "gen.load_state_dict(torch.load(\"{}\".format(out_file)))\n",
    "gen.train() # Basically, we don't want to use eval mode for GANs, due to the batchnorm layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da493fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform inference for a random batch of Pokemon types\n",
    "\n",
    "\n",
    "test_batch_size = 96 # this should be larger in order to have accurate batchnorm layers\n",
    "samples_to_show = 10\n",
    "\n",
    "if use_unet_image:\n",
    "    sample_vector = get_image_noise(test_batch_size, gen_input_image_size, device, upsampling = use_noise_upsampling,noise_dim = original_noise_dim) #get_noise(cur_batch_size, z_dim, device=device)\n",
    "else:\n",
    "    sample_vector = get_noise(test_batch_size, z_dim, device=device)    \n",
    "\n",
    "random_classes = torch.randint(low = 0, high = num_pkmn_types, size = (test_batch_size,)).to(device)\n",
    "fake_images = gen(sample_vector, random_classes)\n",
    "\n",
    "\n",
    "show_tensor_images(fake_images, num_images=samples_to_show,      \n",
    "                   size=(1,256, 256), \n",
    "                   use_uniform_transform = False, \n",
    "                   denorm_transform = denorm_transform)\n",
    "\n",
    "\n",
    "pkmn_classes = [class_idx_to_type[class_idx] for class_idx in random_classes.cpu().numpy()]\n",
    "print(\"Pokemon types we are trying to generate are: {} \\n    \\t \\t \\t \\t \\t {}\".format(pkmn_classes[0:5], pkmn_classes[5:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f9caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform inference for a specific Pokemon type\n",
    "\n",
    "test_batch_size = 96 # this should be larger in order to have accurate batchnorm layers\n",
    "samples_to_show = 10\n",
    "\n",
    "target_type = 'Dragon'\n",
    "target_class_idx = valid_types.index(target_type)\n",
    "\n",
    "if use_unet_image:\n",
    "    sample_vector = get_image_noise(test_batch_size, gen_input_image_size, device, upsampling = use_noise_upsampling,noise_dim = original_noise_dim) #get_noise(cur_batch_size, z_dim, device=device)\n",
    "else:\n",
    "    sample_vector = get_noise(test_batch_size, z_dim, device=device)    \n",
    "\n",
    "random_classes = torch.randint(low = 0, high = num_pkmn_types, size = (test_batch_size,)).to(device)\n",
    "fake_images = gen(sample_vector, random_classes)\n",
    "\n",
    "type_class_mask = random_classes == target_class_idx\n",
    "\n",
    "fake_images_of_target_type = fake_images[type_class_mask]\n",
    "target_classes = random_classes[type_class_mask]\n",
    "\n",
    "\n",
    "show_tensor_images(fake_images_of_target_type, num_images=samples_to_show,      \n",
    "                   size=(1,256, 256), \n",
    "                   use_uniform_transform = False, \n",
    "                   denorm_transform = denorm_transform)\n",
    "\n",
    "\n",
    "pkmn_classes = [class_idx_to_type[class_idx] for class_idx in target_classes.cpu().numpy()]\n",
    "print(\"Pokemon types we are trying to generate are: {} \\n    \\t \\t \\t \\t \\t {}\".format(pkmn_classes[0:5], pkmn_classes[5:10]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476b84f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
